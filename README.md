<!-- badges: start -->
[![R build status](https://github.com/mdneuzerling/DrakeModelling/workflows/R-CMD-check/badge.svg)](https://github.com/mdneuzerling/DrakeModelling)
<!-- badges: end -->

# DrakeModelling

[drake](https://github.com/ropensci/drake) is an excellent tool for managing large and complicated workflows. It is the R equivalent of the `make` command, with a particular emphasis on using R functions.

## Concept

A machine learning model might consist of three major components:

1) Exploratory data analysis (EDA)
2) Model training
3) Model execution

These components are created sequentially, but in practice they are **run independently of each other**. EDA is a largely human task, and is usually only performed when the model is created or updated in some major way. The other two components need not operate together --- if model retraining is expensive, or new training data is infrequently available, we might retrain a model on some monthly basis while scoring new data on a daily basis.

This repository attempts to piece together a machine learning model that implements these three components using R-specific tools:

1) EDA --- **R Markdown**
2) Model training --- **drake**
3) Model execution --- **drake**

All three of these components might use similar functions. Typically we would place all of these "helper functions" in a directory (almost always called `R/`) and `source` them as needed. Here I want to try to combine these components into a custom R package.

R packages are the standard for complicated R projects, and machine learning model training and execution certainly meets that criteria. With packages, we gain access to the comprehensive `R CMD CHECK`, as well as `testthat` unit tests and `roxygen2` documentation.

I intend for this repository to serve as a template for my future machine learning projects.

This template uses a simple random forest sentiment analysis model, based on [labelled data available from the UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences). Drake takes care of the data caching for us. This means that we can, say, adjust the hyperparameters of our model and rerun the training plan, and only the modelling step and onwards will be rerun.

This template considers machine learning workflows intended to be executed in batch --- for models that run as APIs, consider using `plumber`.

## Training and execution

After cloning the repo, navigate to the directory in which the files are located. The easiest way to do this is to open the project in RStudio.

Model training and execution plans are generated by functions in the package. The package doesn't actually need to be installed --- we can use `devtools::load_all()` to simulate the installation. The model can be trained with:

```
devtools::load_all()
drake::make(model_training_plan())
```

![](inst/img/drake-model-training-plan.png)

Model execution is run similarly:

```
devtools::load_all()
drake::make(model_execution_plan())
```

![](inst/img/drake-model-execution-plan.png)

Model artefacts --- the random forest model, the vectoriser, and the tfidf weightings --- are saved to and loaded from the `inst/artefacts/` directory. This is an arbitrary choice. We could just as easily use a different directory or remote storage.

Predictions are "submitted" through the `submit_prediction()` function. This function does nothing except sleep for 5 seconds. In practice we would submit model output wherever it needs to go --- locally, a cloud service, etc.

The exploratory data analysis piece can be found in the `inst/eda/` directory. It is compiled with `knitr`.

## Model and prediction verification

Both training and execution plans include a _verification_ step. These are functions that --- using the `assertthat` package --- ensure certain basic facts about the model and its predictions are true. If any of these assertions is false, an error is returned.

The model artefacts and predictions cannot be exported without passing this verification step. Their relevant drake targets are triggered by a change in the md5 hash produced by the verification functions --- a hash that is not produced if an assertion fails.

The assertions I've included here are very basic. However, I think these steps of the plans are important and extensible. We could assert that a model:

* produces sensible outputs, based on type or domain.
* has an accuracy above a given threshold, based on one or more metrics.
* does not produce outputs that are biased against a particular group.

We could also assert that predictions of new data:

* are sensible.
* do not contain sensitive data.
* are not biased against particular groups.

